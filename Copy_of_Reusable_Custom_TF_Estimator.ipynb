{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Reusable Custom TF Estimator",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "G00_DDh5KTNH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ]
    },
    {
      "metadata": {
        "id": "CuUHrk7aPU54",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "`FIO`_ (Feature Input / Output): an experimental library for a unified interface\n",
        "  to read / write (Sequence)Examples to TF Records.\n",
        "\n",
        ".._https://pypi.org/project/fio/\n",
        "\n",
        "A colab demonstrating FIO can be found here: https://colab.research.google.com/drive/1HrSYF1I7rBGaNQ7388Ss3epWPLTloEC6\n",
        "'''\n",
        "!pip install -q fio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NGfKZolJKFMq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import os, sys, json\n",
        "from random import shuffle\n",
        "\n",
        "# use Feature Import/Output to have unified schema\n",
        "from fio import FIO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jVYyt6VlN7hL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ]
    },
    {
      "metadata": {
        "id": "zTni0j11N7rj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Here we define all the constants in relation to the model flow \n",
        "i.e. things related to the data the model handels rather than how the \n",
        "model is constructed\n",
        "\n",
        "In this example we are undertaking a multilabel problem with a sequence of\n",
        "shape [SEQUENCE_LENGTH, SEQUENCE_CHANNELS] and we working with NUMBER_OF_LABELS \n",
        "labels ([SEQUENCE_LENGTH, NUMBER_OF_LABELS])\n",
        "'''\n",
        "SEQUENCE_LENGTH = 20\n",
        "SEQUENCE_CHANNELS = 7\n",
        "NUMBER_OF_LABELS = 4\n",
        "NUMBER_OF_EXAMPLES = 100\n",
        "\n",
        "'''\n",
        "SCHEMA is a concept from the fio library to have a unified tf record \n",
        "write / read interface.\n",
        "\n",
        "All the features of our data are defined here (even if they are not used in the\n",
        "model).\n",
        "'''\n",
        "SCHEMA = {\n",
        "    \n",
        "    'Name': {'length': 'fixed', 'dtype': tf.string,  'shape': []},\n",
        "    \n",
        "    'Sequence': {\n",
        "        'length': 'fixed',\n",
        "        'dtype': tf.float32,\n",
        "        'shape': [SEQUENCE_LENGTH, SEQUENCE_CHANNELS],\n",
        "        'encode': 'channels',\n",
        "        'data_format': 'channels_last'\n",
        "    },\n",
        "    \n",
        "    'Labels': {\n",
        "        'length': 'fixed',\n",
        "        'dtype': tf.float32,\n",
        "        'shape': [SEQUENCE_LENGTH, NUMBER_OF_LABELS],\n",
        "        'encode': 'channels',\n",
        "        'data_format': 'channels_last'\n",
        "    }\n",
        "}\n",
        "\n",
        "# which features from our SCHEMA are the input / output\n",
        "I_FEATURE = 'Sequence'\n",
        "O_FEATURE = 'Labels'\n",
        "\n",
        "# the input and output types\n",
        "I_DTYPE = SCHEMA[I_FEATURE]['dtype']\n",
        "O_DTYPE = SCHEMA[O_FEATURE]['dtype']\n",
        "\n",
        "# function to get the input / output shapes. Since these are dependent on the \n",
        "# batch size, and that might change per experiment, these are lambdas\n",
        "I_SHAPE = lambda bs: (bs, SCHEMA[I_FEATURE][\"shape\"][0],  SCHEMA[I_FEATURE][\"shape\"][1])\n",
        "O_SHAPE = lambda bs: (bs, SCHEMA[O_FEATURE][\"shape\"][0],  SCHEMA[O_FEATURE][\"shape\"][1])\n",
        "\n",
        "fio = FIO(\n",
        "    schema = SCHEMA,\n",
        "    etype = 'sequence_example',\n",
        "    sequence_features = [I_FEATURE, O_FEATURE]\n",
        ")\n",
        "\n",
        "MODEL_DIR = './test'\n",
        " \n",
        "\n",
        "# all of the file names (write one example per record)\n",
        "FILE_NAMES = [f'sequence_{i}.tfrecord' for i in range(NUMBER_OF_EXAMPLES)]\n",
        "\n",
        "# how to partition our datset into train, validation and test sets\n",
        "DATA_RATIOS = [0.7, 0.2, 0.1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q5Ykg8IXTdap",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data"
      ]
    },
    {
      "metadata": {
        "id": "sMvGQHmPUPJU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ]
    },
    {
      "metadata": {
        "id": "_-JdAaeKTdqa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tf_type_string(tf_type:str): \n",
        "  return str(tf_type).replace(\"<dtype: \\'\", '').replace(\"\\'>\", '')\n",
        "\n",
        "\n",
        "def random_encode_multilabels(array:list, number_of_labels:int):\n",
        "  labels = [\n",
        "      [1 for i in range(number_of_non_zeros)] + \n",
        "      [0 for i in range(number_of_labels - number_of_non_zeros)] \n",
        "      for number_of_non_zeros in array\n",
        "  ]\n",
        "  from random import shuffle\n",
        "  for l in labels:\n",
        "    shuffle(l)\n",
        "  return labels\n",
        "\n",
        "def random_multilabels(shape):\n",
        "  batch_size, length, labels = shape\n",
        "  return np.array([\n",
        "      random_encode_multilabels(num_nonzeros, labels) for num_nonzeros in \n",
        "      np.random.randint(1, labels, (batch_size, length))\n",
        "  ])\n",
        "\n",
        "def random_encode_onehot(array:list, number_of_classes:int):\n",
        "  hot = [[0 for i in range(number_of_classes)] for element in array]\n",
        "  for which, encoded in enumerate(hot):\n",
        "    encoded[array[which]] = 1\n",
        "  return hot\n",
        "\n",
        "def random_onehot(shape):\n",
        "  batch_size, length, channels = shape\n",
        "  return np.array([\n",
        "      random_encode_onehot(hot_channel, channels) for hot_channel in \n",
        "      np.random.randint(0, channels-1, (batch_size, length))\n",
        "  ])\n",
        "\n",
        "def partition_files(files, train=1, valid=0, test=0):\n",
        "  n = len(files)\n",
        "  \n",
        "  shuffle(files)\n",
        "  \n",
        "  a = int(n * (train))\n",
        "  b = int(n * (train + valid))\n",
        "  c = int(n * (train + valid + test))\n",
        "  \n",
        "  return {\n",
        "      'train': files[:a],\n",
        "      'valid': files[a:b],\n",
        "      'test':  files[b:]\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yyEAb7KPWXta",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make dummy data"
      ]
    },
    {
      "metadata": {
        "id": "KU9w2qoZUTDZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# our sequences are fixed length with binary channels that, in this case,\n",
        "# happen to never be 1 at the same instance\n",
        "sequences = random_onehot((NUMBER_OF_EXAMPLES, SEQUENCE_LENGTH, SEQUENCE_CHANNELS))\\\n",
        "            .astype(tf_type_string(I_DTYPE))\n",
        "  \n",
        "# randomly make multilabels\n",
        "seqlabels = random_multilabels((NUMBER_OF_EXAMPLES, SEQUENCE_LENGTH, NUMBER_OF_LABELS))\\\n",
        "            .astype(tf_type_string(O_DTYPE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PJePi0Ei3oac",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### see what data looks like"
      ]
    },
    {
      "metadata": {
        "id": "5ogk0FHq3olZ",
        "colab_type": "code",
        "outputId": "b767471d-f05c-4834-841f-67aba5111747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "cell_type": "code",
      "source": [
        "# (features, labels)\n",
        "(sequences[0], seqlabels[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.]], dtype=float32), array([[0., 1., 1., 1.],\n",
              "        [1., 1., 0., 1.],\n",
              "        [1., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [1., 1., 0., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 1., 0., 1.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [1., 1., 0., 1.],\n",
              "        [0., 1., 0., 1.],\n",
              "        [1., 0., 0., 1.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 1., 1., 0.],\n",
              "        [1., 0., 0., 1.],\n",
              "        [0., 1., 1., 1.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 0., 1., 1.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 0., 0., 1.],\n",
              "        [1., 0., 1., 0.]], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "T1AEaE9sWYQl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Write dummy data to TF Records"
      ]
    },
    {
      "metadata": {
        "id": "bHmwIGsRUlXL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(NUMBER_OF_EXAMPLES):\n",
        "  # give our examples a name\n",
        "  name = f'sequence_{i}'\n",
        "  \n",
        "  schema = {'Name': name, 'Sequence': sequences[i], 'Labels': seqlabels[i]}\n",
        "  \n",
        "  # writing tf records is so much easier like this\n",
        "  example = fio.to_example(schema)\n",
        "  \n",
        "  # each example gets a record\n",
        "  with tf.python_io.TFRecordWriter(f'{name}.tfrecord') as writer:\n",
        "    writer.write(example.SerializeToString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oltvIB36Yupk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Split data"
      ]
    },
    {
      "metadata": {
        "id": "JkgM69VbZp6z",
        "colab_type": "code",
        "outputId": "7747b477-cbcb-45ce-abd8-2bdb3f8a682a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "DATASET is a dictionary of keys 'train', 'valid', and test', each of which \n",
        "corresponds to a list of strings indicating full paths to TF Record files.\n",
        "'''\n",
        "DATASET = partition_files(FILE_NAMES, *DATA_RATIOS)\n",
        "\n",
        "print('DS\\t# example')\n",
        "for key in DATASET:\n",
        "  print(key, len(DATASET[key]), sep='\\t')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DS\t# example\n",
            "train\t70\n",
            "valid\t19\n",
            "test\t11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7hcTmb6yMJP9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Architecture Functions"
      ]
    },
    {
      "metadata": {
        "id": "brDPqN-NMMWJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss fn"
      ]
    },
    {
      "metadata": {
        "id": "Pzlgb_EoMMgT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def multilabel_loss(outputs, targets):\n",
        "    # Note: sigmoid_cross_entropy_with_logits applies sigmoid to the logits\n",
        "    with tf.variable_scope('multilabel_loss'):\n",
        "        return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=targets, logits=outputs))\n",
        "\n",
        "  \n",
        "  \n",
        "def loss_fn(model):\n",
        "    \"\"\"How to calculate the loss of the model.\n",
        "\n",
        "    Args:\n",
        "        model (dict): a `dict` containing the model\n",
        "\n",
        "    Returns:\n",
        "        model (dict): an updated `dict` containing the loss of the model\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # here you extract what you need to calculate the loss\n",
        "    logits = model['net_outputs']\n",
        "    labels = model['labels']\n",
        "\n",
        "    # here you calculate the loss\n",
        "    loss = multilabel_loss(logits, labels)\n",
        "\n",
        "    # add loss to model\n",
        "    model['loss'] = loss\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eMC8RMIqKWlt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Estimator Functions"
      ]
    },
    {
      "metadata": {
        "id": "YEuxlZp-Kmqz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## build_fn"
      ]
    },
    {
      "metadata": {
        "id": "JkogBGlJKqQt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv1d_to_labels(inputs, labels:int, kernel_size:int):\n",
        "    '''\n",
        "    Takes <inputs> tensor and calls conv1d with <labels> number of filters using\n",
        "    <kernels_size> and padding='same'. Ideal to reshape <input> into correct\n",
        "    shape for semantic segmentation problems\n",
        "    '''\n",
        "    with tf.variable_scope('conv1d_to_labels'):\n",
        "        x = tf.layers.conv1d(inputs, labels, kernel_size, name=\"conv\", padding=\"same\", reuse=tf.AUTO_REUSE)\n",
        "        return x\n",
        "\n",
        "\n",
        "def build_fn(model):\n",
        "  '''\n",
        "  build_fn serves to construct the architecture / wire the network for all by the\n",
        "  last activation function.\n",
        "  '''\n",
        "\n",
        "  # here we extract what is needed for building the graph\n",
        "  features = model['features']\n",
        "  params   = model['params']\n",
        "  x = features['input_tensors']\n",
        "  \n",
        "  # NEEDED because TF Records are the worst\n",
        "  # x.set_shape(I_SHAPE(params['batch_size']))  \n",
        "  if model['mode'] != 'predict':\n",
        "    x.set_shape(I_SHAPE(None))\n",
        " \n",
        "  # here you wire how features go throught the graph\n",
        "  # in this toy example, we just reshape (via a convolution) to match the labels\n",
        "  x = conv1d_to_labels(x, NUMBER_OF_LABELS, params['kernel'])\n",
        "\n",
        "  # here you store the outputs of the graph\n",
        "  model['net_outputs'] = x # our loss automatically applies the sigmoid for us\n",
        "  model['net_logits']  = tf.nn.sigmoid(x) # <--- the actual logits\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SulLTdzg46lp",
        "colab_type": "code",
        "outputId": "6fe137ec-a1b0-4322-d0fd-04e0e276db24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "tf.data.TFRecordDataset(FILE_NAMES)                                   \\ # dataset from files\n",
        ".map(lambda record: fio.from_record(record))                          \\ # use the schema defined once to load from tf records\n",
        ".map(lambda context, features: fio.reconstitute((context, features))) \\ # undo the dumb forced wrapping of tf records\n",
        ".batch(2).make_one_shot_iterator().get_next()                         \\ # set batch and make iterator\n",
        "'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntf.data.TFRecordDataset(FILE_NAMES)                                   \\\\ # dataset from files\\n.map(lambda record: fio.from_record(record))                          \\\\ # use the schema defined once to load from tf records\\n.map(lambda context, features: fio.reconstitute((context, features))) \\\\ # undo the dumb forced wrapping of tf records\\n.batch(2).make_one_shot_iterator().get_next()                         \\\\ # set batch and make iterator\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "Pb2gbsZCKZbW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## input_fn"
      ]
    },
    {
      "metadata": {
        "id": "_lSkUxvRKSD1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def input_fn(filenames:list, params):\n",
        "  mode = params['mode'] if 'mode' in params else 'train'\n",
        "  batch_size = params['batch_size']\n",
        " \n",
        "  \n",
        "  shuffle(filenames) # <--- far more efficient than tf dataset shuffle\n",
        "  dataset = tf.data.TFRecordDataset(filenames)\n",
        "  \n",
        "  # using fio's SCHEMA fill the TF Feature placeholders with values\n",
        "  dataset = dataset.map(lambda record: fio.from_record(record))\n",
        "  \n",
        "  # using fio's SCHEMA restructure and unwrap (if possible) features (because tf records require wrapping everything into a list)\n",
        "  dataset = dataset.map(lambda context, features: fio.reconstitute((context, features)))\n",
        "  \n",
        "  # dataset should be a tuple of (features, labels)\n",
        "  dataset = dataset.map(lambda context, features: ( \n",
        "      {\"input_tensors\": features[I_FEATURE]}, # features\n",
        "      features[O_FEATURE]                     # labels\n",
        "    )\n",
        "  ) \n",
        "  \n",
        "  \n",
        "  if mode == 'train':\n",
        "    # during evaluation, we do not want to repeat forever\n",
        "    dataset = dataset.repeat()\n",
        "    \n",
        "  # dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n",
        "  \n",
        "  return dataset.make_one_shot_iterator().get_next()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t1WNohNQaWaX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## mode_fns"
      ]
    },
    {
      "metadata": {
        "id": "qS8TXKRCahIp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### train"
      ]
    },
    {
      "metadata": {
        "id": "5qCQmOg9aXEQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mode_train(model):\n",
        "    \"\"\"How to train the model.\n",
        "\n",
        "    Args:\n",
        "        model (dict): a `dict` containing the model\n",
        "\n",
        "    Returns:\n",
        "        spec (`EstimatorSpec`_): Ops and objects returned from a model_fn and passed to an Estimator\n",
        "\n",
        "    .. _EstimatorSpec:\n",
        "        https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec\n",
        "\n",
        "    \"\"\"\n",
        "    # extract variables for easier reading here\n",
        "    global_step   = tf.train.get_global_step()\n",
        "    learning_rate = model['params']['learning_rate']\n",
        "    loss          = model['loss']\n",
        "\n",
        "    # do the training here\n",
        "    model['optimizer'] = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
        "    model['train_op'] = model['optimizer'].minimize(loss, global_step=global_step)\n",
        "\n",
        "    spec = tf.estimator.EstimatorSpec(\n",
        "        mode            = model['mode'],\n",
        "        loss            = model['loss'],\n",
        "        train_op        = model['train_op'],\n",
        "        eval_metric_ops = model['metrics'],\n",
        "        predictions     = model['predictions'],\n",
        "        export_outputs  = model['export_outputs']\n",
        "    )\n",
        "    return spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yITb6nSYaZa_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### eval"
      ]
    },
    {
      "metadata": {
        "id": "t2r8tk6LaZn2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mode_eval(model:dict):\n",
        "    \"\"\"How to evaluate the model.\n",
        "\n",
        "    Args:\n",
        "        model (dict): a `dict` containing the model\n",
        "\n",
        "    Returns:\n",
        "        spec (`EstimatorSpec`_): Ops and objects returned from a model_fn and passed to an Estimator\n",
        "\n",
        "    .. _EstimatorSpec:\n",
        "        https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec\n",
        "\n",
        "    \"\"\"\n",
        "    # do the evaluation here\n",
        "\n",
        "    spec = tf.estimator.EstimatorSpec(\n",
        "        mode            = model['mode'],\n",
        "        loss            = model['loss'],\n",
        "        eval_metric_ops = model['metrics'],\n",
        "        predictions     = model['predictions'],\n",
        "        export_outputs  = model['export_outputs']\n",
        "    )\n",
        "    return spec\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BCpxt8fhaknZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### predict"
      ]
    },
    {
      "metadata": {
        "id": "-76u30KeakyS",
        "colab_type": "code",
        "outputId": "5803d0e3-7e93-44cd-dba0-110cb6e67b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "def mode_predict(model):\n",
        "    \"\"\"How to predict given the model.\n",
        "\n",
        "    Args:\n",
        "        model (dict): a `dict` containing the model\n",
        "\n",
        "    Returns:\n",
        "        spec (`EstimatorSpec`_): Ops and objects returned from a model_fn and passed to an Estimator\n",
        "\n",
        "    .. _EstimatorSpec:\n",
        "        https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec\n",
        "\n",
        "    \"\"\"\n",
        "    # do the predictions here\n",
        "\n",
        "    spec = tf.estimator.EstimatorSpec(\n",
        "        mode           = model['mode'],\n",
        "        predictions    = model['predictions'],\n",
        "        export_outputs = model['export_outputs']\n",
        "    )\n",
        "    return spec\n",
        "\n",
        "  \n",
        "  \n",
        "'''\n",
        "NOTE:\n",
        "try the following mode_predict function and then in `model_fn` replace\n",
        "  \n",
        "   if mode == tf.estimator.ModeKeys.PREDICT: \n",
        "      return mode_predict(MODEL)\n",
        "      \n",
        " with\n",
        " \n",
        "   mode_predict(MODEL)\n",
        "   if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      return MODEL['PREDICT_SPEC']\n",
        "      \n",
        "\n",
        "This will result in an error when running train_and_evaluate because the spec\n",
        "will trigger when initiated, rather than when returned.\n",
        "      \n",
        " -----------------------\n",
        " \n",
        "def mode_predict(model):\n",
        "    # do the predictions here\n",
        "    model['predictions'] = {'labels': model['net_logits']}\n",
        "\n",
        "    model['export_outputs'] = {\n",
        "        k: tf.estimator.export.PredictOutput(v) for k, v in model['predictions'].items()\n",
        "    }\n",
        "    \n",
        "    spec = tf.estimator.EstimatorSpec(\n",
        "        mode           = model['mode'],\n",
        "        predictions    = model['predictions'],\n",
        "        export_outputs = model['export_outputs']\n",
        "    )\n",
        "    model['PREDICT_SPEC'] = spec\n",
        "    return model\n",
        "\n",
        "'''\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nNOTE:\\ntry the following mode_predict function and then in `model_fn` replace\\n  \\n   if mode == tf.estimator.ModeKeys.PREDICT: \\n      return mode_predict(MODEL)\\n      \\n with\\n \\n   mode_predict(MODEL)\\n   if mode == tf.estimator.ModeKeys.PREDICT:\\n      return MODEL['PREDICT_SPEC']\\n      \\n\\nThis will result in an error when running train_and_evaluate because the spec\\nwill trigger when initiated, rather than when returned.\\n      \\n -----------------------\\n \\ndef mode_predict(model):\\n    # do the predictions here\\n    model['predictions'] = {'labels': model['net_logits']}\\n\\n    model['export_outputs'] = {\\n        k: tf.estimator.export.PredictOutput(v) for k, v in model['predictions'].items()\\n    }\\n    \\n    spec = tf.estimator.EstimatorSpec(\\n        mode           = model['mode'],\\n        predictions    = model['predictions'],\\n        export_outputs = model['export_outputs']\\n    )\\n    model['PREDICT_SPEC'] = spec\\n    return model\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "tlbXqRF0an0u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## metrics_fn"
      ]
    },
    {
      "metadata": {
        "id": "biRfL8glaoQo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def metrics_fn(model):\n",
        "    \"\"\"Produce metrics of the model to monitor during training.\n",
        "\n",
        "    Args:\n",
        "        model (dict): a `dict` containing the model\n",
        "\n",
        "    Returns:\n",
        "        model (`dict`): an update `dict` containg the metrics\n",
        "\n",
        "    .. _EstimatorSpec:\n",
        "        https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec\n",
        "\n",
        "    \"\"\"\n",
        "    # here you extract variables for easier reading\n",
        "    labels = model['labels']\n",
        "    predicted = model['predictions']['labels']\n",
        "\n",
        "    # here you calculate your metrics\n",
        "    mae = tf.metrics.mean_absolute_error(labels=labels, predictions=predicted, name='mea_op')\n",
        "    mse = tf.metrics.mean_squared_error(labels=labels, predictions=predicted, name='mse_op')\n",
        "\n",
        "    # here you add your metrics (or anything else) to tf.summary to be monitored\n",
        "    tf.summary.scalar('mae', mae[1])\n",
        "    tf.summary.scalar('mse', mse[1])\n",
        "\n",
        "    # here you add the above metrics to the model\n",
        "    metrics = {'mae': mae, 'mse': mse}\n",
        "    model['metrics'] = metrics\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jxY4HfZXKcv6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## model_fn"
      ]
    },
    {
      "metadata": {
        "id": "A7kC7SdzKnSo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "    MODEL = {'features': features, 'labels': labels, 'mode': mode, 'params': params}\n",
        "\n",
        "    # send the features through the graph\n",
        "    MODEL = build_fn(MODEL)\n",
        "\n",
        "    # prediction\n",
        "    MODEL['predictions'] = {'labels': MODEL['net_logits']}\n",
        "\n",
        "    MODEL['export_outputs'] = {\n",
        "        k: tf.estimator.export.PredictOutput(v) for k, v in MODEL['predictions'].items()\n",
        "    }\n",
        "\n",
        "    \n",
        "    if mode == tf.estimator.ModeKeys.PREDICT: \n",
        "      return mode_predict(MODEL)\n",
        "\n",
        "    # calculate the loss\n",
        "    MODEL = loss_fn(MODEL)\n",
        "\n",
        "    # calculate all metrics and send them to tf.summary\n",
        "    MODEL = metrics_fn(MODEL)\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.EVAL: \n",
        "      return mode_eval(MODEL)\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN: \n",
        "      return mode_train(MODEL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L7HFkPPi1Iha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note: ideally the `MODEL['predictions']` / `MODEL['export_outputs']` would be done in `mode_predict` function, which would return the model along with the prediction `spec`. However, the instant a estimator spec is made, (regardless of scope it seems), it will evalulate that spec. So that is silly."
      ]
    },
    {
      "metadata": {
        "id": "ULw0iI1y7zqY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## serving_fn"
      ]
    },
    {
      "metadata": {
        "id": "dSTiDpgvKeIJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "serving_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(\n",
        "    {'input_tensors': tf.placeholder(tf.float32, I_SHAPE(None), name=\"input_tensors\")})\n",
        "\n",
        "def serving_input_receiver_fn():\n",
        "  input_tensors = tf.placeholder(tf.float32, I_SHAPE(None), name=\"input_tensors\")\n",
        "\n",
        "\n",
        "  features = {'input_tensors' : input_tensors} # this is the dict that is then passed as \"features\" parameter to your model_fn\n",
        "  receiver_tensors = {'input_tensors': input_tensors} # As far as I understand this is needed to map the input to a name you can retrieve later\n",
        "  return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "igskbB71XEpL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run Config"
      ]
    },
    {
      "metadata": {
        "id": "IqfskaOPXE4j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "run_config = tf.estimator.RunConfig(**{\n",
        "    \"model_dir\": MODEL_DIR,\n",
        "    \"keep_checkpoint_max\": 5\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ykxae4VLXNOO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run Params"
      ]
    },
    {
      "metadata": {
        "id": "gOtIpAL6XNp1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "run_params = {\n",
        "    \"batch_size\": 5,\n",
        "    \"kernel\": 3,\n",
        "    \"learning_rate\": 0.001,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gjMARkPnKe15",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Estimator"
      ]
    },
    {
      "metadata": {
        "id": "9BlhcRmyccst",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## init estimator"
      ]
    },
    {
      "metadata": {
        "id": "IOXTadwdWsxL",
        "colab_type": "code",
        "outputId": "3e444b8f-854c-4251-b8ab-0f26470a17ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "est = tf.estimator.Estimator(\n",
        "    model_fn = model_fn,\n",
        "    config = run_config,\n",
        "    params = run_params,\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': './test', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4dd1f281d0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sB-39Bp-7A_O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## define exporter"
      ]
    },
    {
      "metadata": {
        "id": "xGQI18i47BHQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from previous S.O. Question: \n",
        "# https://stackoverflow.com/questions/52641737/tensorflow-1-10-custom-estimator-early-stopping-with-train-and-evaluate\n",
        "\n",
        "exporter = tf.estimator.BestExporter(\n",
        "    name=\"best_exporter\",\n",
        "    serving_input_receiver_fn=serving_input_receiver_fn,\n",
        "    # event_file_pattern=\"model_*\", # <--- doesn't do anything?\n",
        "    exports_to_keep=5\n",
        ") # this will keep the 5 best checkpoints\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6dA4FIs-cjte",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## define train and eval spec"
      ]
    },
    {
      "metadata": {
        "id": "KmHoMpsDaOix",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "133fd395-6c33-49b8-bad3-e6629b2ddb7b"
      },
      "cell_type": "code",
      "source": [
        "eval_run_params = {**run_params, 'mode': 'eval'}\n",
        "\n",
        "train_fn = lambda: input_fn(DATASET['train'], run_params)\n",
        "valid_fn = lambda: input_fn(DATASET['valid'], eval_run_params)\n",
        "test_fn  = lambda: input_fn(DATASET['test'],  eval_run_params)\n",
        "\n",
        "\n",
        "early_stop = tf.contrib.estimator.stop_if_no_decrease_hook(est, 'loss', 10)\n",
        "\n",
        "train_spec = tf.estimator.TrainSpec(input_fn=train_fn, max_steps=200, hooks=[early_stop])\n",
        "eval_spec  = tf.estimator.EvalSpec( input_fn=valid_fn, exporters=exporter)  \n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e1CG7vgWcxB4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run"
      ]
    },
    {
      "metadata": {
        "id": "bRaPjeKJaT_S",
        "colab_type": "code",
        "outputId": "3b5b125e-1f1a-45c6-ab3d-53965fcf9d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "cell_type": "code",
      "source": [
        "tf.estimator.train_and_evaluate(est, train_spec, eval_spec)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-12-7d29de11592e>:28: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n",
            "WARNING:tensorflow:From <ipython-input-12-7d29de11592e>:30: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From <ipython-input-10-6a6e399326c9>:8: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv1d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py:363: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into ./test/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.70050234, step = 1\n",
            "INFO:tensorflow:global_step/sec: 317.631\n",
            "INFO:tensorflow:loss = 0.72646433, step = 101 (0.316 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 200 into ./test/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-02-07T22:55:10Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./test/model.ckpt-200\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2019-02-07-22:55:11\n",
            "INFO:tensorflow:Saving dict for global step 200: global_step = 200, loss = 0.72876316, mae = 0.5069164, mse = 0.26700932\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: ./test/model.ckpt-200\n",
            "INFO:tensorflow:Loading best metric from event files.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/summary_iterator.py:68: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "INFO:tensorflow:Loss for final step: 0.71549654.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'global_step': 200, 'loss': 0.72876316, 'mae': 0.5069164, 'mse': 0.26700932},\n",
              " [None])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "FtHlAC5I7I5G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## export saved model"
      ]
    },
    {
      "metadata": {
        "id": "EHpNvzt-aZma",
        "colab_type": "code",
        "outputId": "d87872ba-f4ff-4a8d-ddc6-15197156395f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "cell_type": "code",
      "source": [
        "est.export_savedmodel('./here', serving_input_receiver_fn)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['labels', 'serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:Restoring parameters from ./test/model.ckpt-200\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1020: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Pass your op to the equivalent parameter main_op instead.\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: ./here/temp-b'1549580111'/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'./here/1549580111'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "G_xvpCTnJiij",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# what does \"Pass your op to the equivalent parameter main_op instead.\" mean?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "93SJao6T3xqm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## evaluate with test set"
      ]
    },
    {
      "metadata": {
        "id": "7g6eUAYksaOv",
        "colab_type": "code",
        "outputId": "9b40171b-f65a-4fa3-dceb-9e1eb2637afb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "test_res = est.evaluate(input_fn=test_fn)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-02-07T22:55:11Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from ./test/model.ckpt-200\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2019-02-07-22:55:11\n",
            "INFO:tensorflow:Saving dict for global step 200: global_step = 200, loss = 0.72596467, mae = 0.5061118, mse = 0.26587075\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: ./test/model.ckpt-200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yQe11GK_skH6",
        "colab_type": "code",
        "outputId": "82813de8-663a-4c77-dc84-a6e7437c9873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_res"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'global_step': 200, 'loss': 0.72596467, 'mae': 0.5061118, 'mse': 0.26587075}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "Duw34ARZ31Ne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Predict with trained model"
      ]
    },
    {
      "metadata": {
        "id": "J5jvemC2sqhr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_predict = random_onehot((1, SEQUENCE_LENGTH, SEQUENCE_CHANNELS))\\\n",
        "            .astype(tf_type_string(I_DTYPE))\n",
        "pred_features = {'input_tensors': to_predict}\n",
        "pred_ds = tf.data.Dataset.from_tensor_slices(pred_features)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oP_wNxMHMpXe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predicted = est.predict(lambda: pred_ds, yield_single_examples=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IPyhdwCwdQ-f",
        "colab_type": "code",
        "outputId": "4e536ffb-ca65-42ff-9fda-1b443c2bceef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Why does this throw an error?\n",
        "\n",
        "Also, how would one load the saved model and predict, rather than use the current runtime instance?\n",
        "'''\n",
        "\n",
        "# next(predicted)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWhy does this throw an error?\\n\\nAlso, how would one load the saved model and predict, rather than use the current runtime instance?\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "L8V8lpVFdyWL",
        "colab_type": "code",
        "outputId": "4a27ec56-cfb6-4e3e-e353-d9ad70078fa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "cell_type": "code",
      "source": [
        "to_predict = random_onehot((2, SEQUENCE_LENGTH, SEQUENCE_CHANNELS))\\\n",
        "            .astype(tf_type_string(I_DTYPE))\n",
        "pred_features = {'input_tensors': to_predict}\n",
        "\n",
        "def predict_input_fn(data, batch_size=2):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "  return dataset.batch(batch_size).prefetch(None)\n",
        "\n",
        "predicted = est.predict(lambda: predict_input_fn(pred_features), yield_single_examples=False)\n",
        "next(predicted)\n",
        "# predicted"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/util.py:105: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from ./test/model.ckpt-200\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': array([[[0.5381952 , 0.47535905, 0.53976953, 0.5383521 ],\n",
              "         [0.39776167, 0.61911815, 0.51660883, 0.41811502],\n",
              "         [0.4385622 , 0.31003594, 0.63291687, 0.65643287],\n",
              "         [0.59574735, 0.64704746, 0.39853823, 0.48056987],\n",
              "         [0.4587513 , 0.59398854, 0.53326726, 0.7161053 ],\n",
              "         [0.62046874, 0.38805783, 0.4934168 , 0.50747895],\n",
              "         [0.5889618 , 0.50825185, 0.5316071 , 0.35716754],\n",
              "         [0.4263634 , 0.6326133 , 0.5307879 , 0.40672418],\n",
              "         [0.5778164 , 0.4583077 , 0.54291016, 0.5347045 ],\n",
              "         [0.5778164 , 0.4583077 , 0.54291016, 0.5347045 ],\n",
              "         [0.6939806 , 0.37592912, 0.47117758, 0.56688476],\n",
              "         [0.67693424, 0.28124288, 0.5794838 , 0.5382769 ],\n",
              "         [0.40714478, 0.52797145, 0.636309  , 0.37912387],\n",
              "         [0.3943149 , 0.7378451 , 0.36519176, 0.37444502],\n",
              "         [0.48771715, 0.40642095, 0.56607074, 0.6357933 ],\n",
              "         [0.61365175, 0.47290897, 0.5402438 , 0.55255586],\n",
              "         [0.5271967 , 0.64608914, 0.40530843, 0.4914508 ],\n",
              "         [0.3593592 , 0.70399994, 0.35832208, 0.38199627],\n",
              "         [0.5820206 , 0.500061  , 0.48489392, 0.54616946],\n",
              "         [0.5088835 , 0.53750634, 0.6030136 , 0.4721968 ]],\n",
              " \n",
              "        [[0.49579555, 0.66626835, 0.32207596, 0.4532778 ],\n",
              "         [0.5080927 , 0.53887564, 0.36960822, 0.5088696 ],\n",
              "         [0.45663276, 0.5841784 , 0.5565172 , 0.5137702 ],\n",
              "         [0.46071208, 0.7309965 , 0.3966567 , 0.47185826],\n",
              "         [0.5698767 , 0.4494878 , 0.48342922, 0.62066513],\n",
              "         [0.41983944, 0.4971802 , 0.64966565, 0.5298955 ],\n",
              "         [0.5991428 , 0.38851175, 0.5106519 , 0.486904  ],\n",
              "         [0.6075311 , 0.6625719 , 0.41292745, 0.63144815],\n",
              "         [0.5856091 , 0.3750233 , 0.5500063 , 0.54264677],\n",
              "         [0.53015167, 0.33268023, 0.5526486 , 0.49660924],\n",
              "         [0.5669427 , 0.50944436, 0.34669608, 0.45312157],\n",
              "         [0.48938176, 0.58184594, 0.4876312 , 0.6536959 ],\n",
              "         [0.45641157, 0.6731771 , 0.4433932 , 0.4863089 ],\n",
              "         [0.50975764, 0.70369464, 0.29959637, 0.5283832 ],\n",
              "         [0.5820206 , 0.500061  , 0.48489392, 0.54616946],\n",
              "         [0.46562883, 0.6214287 , 0.59839153, 0.56057763],\n",
              "         [0.3210615 , 0.49080482, 0.6668406 , 0.49855447],\n",
              "         [0.63534594, 0.6450266 , 0.37329835, 0.4623925 ],\n",
              "         [0.5856091 , 0.3750233 , 0.5500063 , 0.54264677],\n",
              "         [0.61641496, 0.359065  , 0.45339152, 0.3969642 ]]], dtype=float32)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "Mon6t--VnZXp",
        "colab_type": "code",
        "outputId": "2003362a-6ad4-4ced-dc51-390239cbca3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.contrib import predictor\n",
        "\n",
        "predict_fn = predictor.from_saved_model('./here/{}'.format(os.listdir('./here')[0]))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
            "INFO:tensorflow:Restoring parameters from ./here/1549580111/variables/variables\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CcKLmHShvx2M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gRW_FKdHRdDW",
        "colab_type": "code",
        "outputId": "0abe93f3-d059-4c25-bf2a-cbdaa6d67337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "cell_type": "code",
      "source": [
        "predict_fn(pred_features)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': array([[[0.5381952 , 0.47535905, 0.53976953, 0.5383521 ],\n",
              "         [0.39776167, 0.61911815, 0.51660883, 0.41811502],\n",
              "         [0.4385622 , 0.31003594, 0.63291687, 0.65643287],\n",
              "         [0.59574735, 0.64704746, 0.39853823, 0.48056987],\n",
              "         [0.4587513 , 0.59398854, 0.53326726, 0.7161053 ],\n",
              "         [0.62046874, 0.38805783, 0.4934168 , 0.50747895],\n",
              "         [0.5889618 , 0.50825185, 0.5316071 , 0.35716754],\n",
              "         [0.4263634 , 0.6326133 , 0.5307879 , 0.40672418],\n",
              "         [0.5778164 , 0.4583077 , 0.54291016, 0.5347045 ],\n",
              "         [0.5778164 , 0.4583077 , 0.54291016, 0.5347045 ],\n",
              "         [0.6939806 , 0.37592912, 0.47117758, 0.56688476],\n",
              "         [0.67693424, 0.28124288, 0.5794838 , 0.5382769 ],\n",
              "         [0.40714478, 0.52797145, 0.636309  , 0.37912387],\n",
              "         [0.3943149 , 0.7378451 , 0.36519176, 0.37444502],\n",
              "         [0.48771715, 0.40642095, 0.56607074, 0.6357933 ],\n",
              "         [0.61365175, 0.47290897, 0.5402438 , 0.55255586],\n",
              "         [0.5271967 , 0.64608914, 0.40530843, 0.4914508 ],\n",
              "         [0.3593592 , 0.70399994, 0.35832208, 0.38199627],\n",
              "         [0.5820206 , 0.500061  , 0.48489392, 0.54616946],\n",
              "         [0.5088835 , 0.53750634, 0.6030136 , 0.4721968 ]],\n",
              " \n",
              "        [[0.49579555, 0.66626835, 0.32207596, 0.4532778 ],\n",
              "         [0.5080927 , 0.53887564, 0.36960822, 0.5088696 ],\n",
              "         [0.45663276, 0.5841784 , 0.5565172 , 0.5137702 ],\n",
              "         [0.46071208, 0.7309965 , 0.3966567 , 0.47185826],\n",
              "         [0.5698767 , 0.4494878 , 0.48342922, 0.62066513],\n",
              "         [0.41983944, 0.4971802 , 0.64966565, 0.5298955 ],\n",
              "         [0.5991428 , 0.38851175, 0.5106519 , 0.486904  ],\n",
              "         [0.6075311 , 0.6625719 , 0.41292745, 0.63144815],\n",
              "         [0.5856091 , 0.3750233 , 0.5500063 , 0.54264677],\n",
              "         [0.53015167, 0.33268023, 0.5526486 , 0.49660924],\n",
              "         [0.5669427 , 0.50944436, 0.34669608, 0.45312157],\n",
              "         [0.48938176, 0.58184594, 0.4876312 , 0.6536959 ],\n",
              "         [0.45641157, 0.6731771 , 0.4433932 , 0.4863089 ],\n",
              "         [0.50975764, 0.70369464, 0.29959637, 0.5283832 ],\n",
              "         [0.5820206 , 0.500061  , 0.48489392, 0.54616946],\n",
              "         [0.46562883, 0.6214287 , 0.59839153, 0.56057763],\n",
              "         [0.3210615 , 0.49080482, 0.6668406 , 0.49855447],\n",
              "         [0.63534594, 0.6450266 , 0.37329835, 0.4623925 ],\n",
              "         [0.5856091 , 0.3750233 , 0.5500063 , 0.54264677],\n",
              "         [0.61641496, 0.359065  , 0.45339152, 0.3969642 ]]], dtype=float32)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "QhPheCxBR_Vr",
        "colab_type": "code",
        "outputId": "605f2d20-93a2-4433-b096-20fea8abe70a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "predict_fn"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SavedModelPredictor with feed tensors {'input_tensors': <tf.Tensor 'input_tensors:0' shape=(?, 20, 7) dtype=float32>} and fetch_tensors {'output': <tf.Tensor 'Sigmoid:0' shape=(?, 20, 4) dtype=float32>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "8GshDx6dv645",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MMC8EGvHjoom",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}